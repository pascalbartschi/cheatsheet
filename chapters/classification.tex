\section*{Classification}

\textbf{Zero-One loss} \quad not convex or continuous

\qquad \qquad $\ell_{0-1}(\hat{f}(x), y) = \mathbb{I}_{y \neq \sgn \hat{f}(x)}$

\textbf{Logistic loss} \quad $\log(1 + e^{-y \hat{f}(x)})$

\textbf{Hinge loss} \quad \ $\max(0, 1-y \hat{f}(x))$

\textbf{Softmax} $p(1 | x) = \frac{1}{1 + e^{- \hat{f}(x)}}, p(-1 | x) = \frac{1}{1 + e^{\hat{f}(x)}}$ 

Multi-Class \ \ \ $\hat{p}_k = e^{\hat{f}_k(x)} / \sum_{i=1}^K e^{\hat{f}_j(x)}$

\subsection*{Linear Classifiers}

$f(x) = w^\top x$, the decision boundary $f(x) = 0$. \smallskip

If data is lin. sep., grad. desc. converges to \textbf{Maximum-Margin Solution}: 

\quad $w_\text{MM} = \text{argmax} \; \text{margin} (w) \; \text{with } ||w||_2 = 1$

Where $\text{margin} (w) = \min_i y_i w^\top x_i$.
 
\subsection*{Support Vector Machines}
\textbf{Hard SVM}

\qquad $\hat{w} = \min_w ||w||_2 \; \; \text{s.t. } \forall i \;y_i w^\top x_i \geq 1$
 
\textbf{Soft SVM} \quad allow "slack" in the constraints
$$\hat{w} = \min_{w, \xi} \frac{1}{2} ||w||_2^2 + \lambda \sum_{i=1}^n \underbrace{\max (0, 1 - y_i w^\top x_i)}_{\text{hinge loss}}$$ \\[-23pt]

\subsection*{Metrics} 

\begin{multicols*}{2}
	\begin{center}
		\includegraphics[width=\columnwidth]{confusion-matrix.jpeg}
	\end{center}
	
	$\text{error}_1 / \text{FPR}: \frac{\text{FP}}{\text{TN + FP}}$
	$\text{error}_2 / \text{FNR}: \frac{\text{FN}}{\text{TP + FN}}$
	$\text{TPR / Recall}: \frac{\text{TP}}{\text{TP + FN}}$
	$\text{TNR}: \frac{\text{TN}}{\text{TN + FP}}$
\end{multicols*}

.\\[-20pt]
$\text{Precision}: \frac{\text{TP}}{\text{TP + FP}} = 1 - \text{FDR}$, FDR = $\frac{FP}{TP + FP}$

\textbf{AUROC}: Plot TPR (y) vs. FPR (x) and compare different ROC's with area under the curve.

\textbf{F1-Score}: $\frac{2TP}{2TP+FP+FN}=\frac{2}{1/Precision+1/Recall}$, $\text{Accuracy}: \frac{\text{TP + TN}}{\text{P + N}} = \frac{TP + TN}{TP + TN + FP + FN}$

Goal: large recall and small FPR.
