\section*{Gradient Descent}
Converges only for convex case.
\[
	w^{t+1} = w^t - \eta_t \cdot \nabla \ell(w^t)
\]

For linear regression:
\[
	||w^t - w^*||_2 \leq ||I - \eta X^\top X||_{op}^t ||w^0 - w^*||_2
\]

$\rho = ||I - \eta X^\top X||_{op}^t$ conv. speed for const. $\eta$. Opt. fixed $\eta = \frac{2}{\lambda_{\text{min}} + \lambda_{\text{max}}}$ and max. $\eta \leq \frac{2}{\lambda_{\text{max}}}$. 

\textbf{Momentum}: $w^{t+1} = w^t + \gamma \Delta w^{t-1} - \eta_t \nabla \ell(w^t)$

\textbf{SGD}: pick point u.a.r.
\textbf{Minibatch SGD}: pick a batch u.a.r., same in expectation