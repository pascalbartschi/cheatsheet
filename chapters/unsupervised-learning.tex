\section*{Unsupervised Learning}

\subsection*{k-Means Clustering}

$\min \hat{R}(\mu) = \sum_{i=1}^n \min_{j\in\{1,...k\}}\|x_i-\mu_j\|_2^2$

Non-convex, NP-hard, . Can be kernelized. \\[-10pt]

$\arg \min_{\substack{W \in \mathbb{R}^{d \times k} \\ \{ z_i \}_{i=1}^{n} \subset E_k}} \sum_{i=1}^{n} \left\| W z_i - x_i \right\|_2^2$




\textbf{Lloyd's heuristic \quad \normalfont \color{black}(closed form)} 

1. assign $z_i = \text{argmin}_{j\in\{1,...,k\}}\|x_i - \mu_j^{t-1}\|_2^2$

2. update $\mu_j^{(t)} = \frac{1}{n_j}\sum_{i:z_i=j}x_i$\\
Monotonically decreases objective (to local min). iter$^{-1}$: $O(nkd)$, worst-case exponential

\textbf{k-Means++}: $p(i) = \frac{1}{z} \min_{l \in \{1,...,j\}} ||x_i - \mu_l||_2^2$

Converges in exp. $\mathcal O (\log k) * \text{opt. solution}$.

\subsection*{Principal Component Analysis}
 
$\arg \min_{\substack{W \in \mathbb{R}^{d \times k} \\ \{ z_i \}_{i=1}^{n} \subset \mathbb{R}^k \\ W^T W = I_k}} \sum_{i=1}^{n} \left\| W z_i - x_i \right\|_2^2
$ (s.t. $\mu = 0$)

with solution $W^* = (v_1|...|v_k)$ where $v_i$ are the ordered 
eigvec. of $\frac{1}{n}\sum_ix_ix_i^\top$ 
and $z_i = {W^*}^\top x_i$. 

\subsection*{PCA through SVD}
\color{White} . \color {Black}\\[-10pt]
$X = U S V^\top \rightarrow \Sigma = \frac{1}{n} X^T X = \frac{1}{n} VS^2 V^T.$

$W_*=V_i$  (i PCs) \& $\lambda_i = \frac{\sigma_i^2}{n} \rightarrow \lambda_i$ is Var(PC$_i$)

\subsection*{Kernel PCA}

Kernel PCs are given by $\alpha^{(1)},...,\alpha^{(k)}\in \mathbb{R}^n$ 
where $\alpha^{(i)} = \frac{1}{\sqrt{\lambda_i}}v_i$ and 
$K = \sum_{i=1}^n \lambda_i v_i v_i^\top$ with ordered $\lambda_i.$ A point 
$x$ is projected to $z \in \mathbb{R}^k$:
$z_i = \sum_{j=1}^n\alpha_j^{(i)}k(x,x_j)$

\subsection*{Autoencoders}

We want to minimize $\frac{1}{n}\sum_{i=1}^n ||x_i - \hat{x}_i||_2^2$.
$$\hat{x} = f_{dec}(f_{enc}(x, \theta_{enc}); \theta_{dec})$$

Lin. activation func. \& square loss $=>$ PCA