\section*{Unsupervised Learning}

\subsection*{k-Means Clustering}

Want $\mu_i$ to minimize $\sum_{i=1}^n \min_{j\in\{1,...k\}}\|x_i-\mu_j\|_2^2$
Non-convex and NP-hard in general. Can be kernelized.

\subsection*{Lloyd's heuristic}
$\hspace*{3mm}z_i = \text{argmin}_{j\in\{1,...,k\}}\|x_i - \mu_j^{t-1}\|_2^2\\
\hspace*{3mm}\mu_j^{(t)} = \frac{1}{n_j}\sum_{i:z_i=j}x_i$\\
Monotonically decreases objective and converges to a local 
optimum. Cost per iteration $O(nkd)$, worst-case exponential

\textbf{k-Means++}: \begin{compactitem}
	\item Random data point $\mu_1 = x_i$
	\item Add $\mu_2,...,\mu_k$ rand., with prob:
		$$\text{given } \mu_{1:j} \text{ pick } \mu_{j+1} = x_i$$ 
		$\text{ where } p(i) = \frac{1}{z} \min_{l \in \{1,...,j\}} ||x_i - \mu_l||_2^2$
\end{compactitem}
Converges in expectation $\mathcal O (\log k) * \text{opt. solution}$.

\subsection*{Principal Component Analysis}

Given centered data, the PCA problem is 
$$\min_{W^\top W=I_k,z_i\in\R^k}\sum_{i=1}^n||W z_i - x_i||_2^2,$$
with solution $W^* = (v_1|...|v_k)$ where $v_i$ are the ordered 
eigvec. of $\frac{1}{n}\sum_ix_ix_i^\top$ 
and $z_i = {W^*}^\top x_i$. 

\subsection*{PCA through SVD}
\color{White} . \color {Black}\\[-10pt]
The first $k$ columns of $V$ where $X = U S V^\top$.

\subsection*{Kernel PCA}

The Kernel Principal Components are given by $\alpha^{(1)},...,\alpha^{(k)}\in \mathbb{R}^n$ 
where $\alpha^{(i)} = \frac{1}{\sqrt{\lambda_i}}v_i$ and 
$K = \sum_{i=1}^n \lambda_i v_i v_i^\top$ with ordered $\lambda_i.$ A point 
$x$ is projected to $z \in \mathbb{R}^k$:
$z_i = \sum_{j=1}^n\alpha_j^{(i)}k(x,x_j)$

\subsection*{Autoencoders}

We want to minimize $\frac{1}{n}\sum_{i=1}^n ||x_i - \hat{x}_i||_2^2$.
$$\hat{x} = f_{dec}(f_{enc}(x, \theta_{enc}); \theta_{dec})$$

Lin. activation func. \& square loss $=>$ PCA