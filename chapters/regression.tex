\section*{Regression}

\textbf{Squared loss} \quad (convex)

\qquad \qquad $\frac{1}{n}\sum (y_i - f(x_i))^2 = \frac{1}{n}||y - X w||_2^2$

\qquad \qquad $\nabla_w L(w) = 2X^\top(Xw -y)$

Solution: $\hat{w} = (X^\top X)^{-1}X^\top y$

\subsection*{Regularization}

\textbf{Lasso Regression} \quad (sparse)

\qquad \qquad $\argmin{w \in \R^d} ||y - \Phi w||_2^2 + \lambda ||w||_1$

\textbf{Ridge Regression}

\qquad \qquad $\argmin{w \in \R^d} ||y - \Phi w||_2^2 + \lambda ||w||_2^2$

\qquad \qquad $\nabla_w L(w) = 2X^\top(Xw -y) + 2 \lambda w$

Solution: $\hat w = (X^\top X + \lambda I)^{-1} X^\top y$

large $\lambda \Rightarrow$ larger bias but smaller variance 

\subsection*{Standardization}
Goal: each feature: $\mu = 0$, unit $\sigma^2$: $\tilde{x}_{i,j} = \frac{(x_{i,j}-\hat{\mu}_j)}{\hat{\sigma}_j}$\\
$\hat{\mu}_j = \frac{1}{n}\sum_{i=1}^n x_{i,j}$, $\hat{\sigma}_j^2 = \frac{1}{n}\sum_{i=1}^n {(x_{i,j}-\hat{\mu}_j)}^2$ 

\subsection*{Cross-Validation}

\begin{rowlist}
	\item For all folds $i = 1,..., k$:  Train $\hat{f}_i$ on $D' - D'_i$; Val. error $R_i = \frac{1}{|D'_i|} \sum \ell(\hat{f}_i(x), y)$
	\item Compute CV error $\frac{1}{k} \sum_{i=1}^k R_i$
	\item Pick model with lowest $CV$ error
\end{rowlist}