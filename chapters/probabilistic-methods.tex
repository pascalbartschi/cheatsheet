\section*{Probabilistic Methods}

Assume $\mathcal{D} = \left\{ (x_i, y_i) \right\}_{i=1}^{n}.$ is i.i.d: $\mathbb{P}_{\mathcal{D}} = \prod_{i=1}^{n} \mathbb{P}_{X_i, Y_i}
$, 
Parametric Family: $\mathbb{P}^* = \mathbb{P}^{\theta*} \in \mathcal{P}.$  ,e.g. $\mathcal{P} = \left\{ \mathcal{N}(\mu, \sigma^2) \mid (\mu, \sigma) \in \mathbb{R}^2 \right\}.$

\subsection*{Statistical Inference}

\textbf{frequentist:} $\mathbb{P}^* \in \mathcal{P} = \left\{ \mathbb{P}; \theta; \theta \in \Theta \right\}$
\textbf{Baysian:} $\mathbb{P}^* = \mathbb{P}_{\cdot \mid \theta} \text{ with } \theta \sim \mathbb{P}_{\theta} \text{ (prior)}$ \\[-15pt]

\[
\underbrace{p( \theta \mid \mathcal{D})}_{\text{posterior}} = \underbrace{p(\mathcal{D} \mid \theta)}_{\text{likelihood}} \underbrace{p(\theta)}_{\text{prior}} / {\underbrace{p(\mathcal{D})}_{\text{evidence}}}
\] 
evidence: $p(\mathcal{D}) = \int p(\mathcal{D} \mid \theta) p(\theta) d\theta$

\subsection*{MLE \quad \color{black}$\arg \max_{\theta \in \Theta} p(\mathcal{D}; \theta)$} 
$\hat{\theta}_{\text{MLE}} \overset{\text{i.i.d.}}{:=} \arg \max_{\theta \in \Theta} \prod_{i=1}^{n} p(x_i, y_i; \theta)$

$\overset{\text{(discr.)}}{=} \arg \min_{\theta \in \Theta} \sum_{i=1}^{n} -\log p(y_i \mid x_i; \theta)$

\subsection*{MAP \quad \color{black}$\arg \max_{\theta \in \Theta} p(\theta \mid \mathcal{D})$}
$\hat{\theta}_{\text{MAP}} \overset{\text{i.i.d.}}{:=} \arg \max_{\theta \in \Theta} \left( \prod_{i=1}^{n} p(x_i, y_i \mid \theta) \right) p(\theta)$

$\overset{\text{discr.}}{=} \arg \min_{\theta \in \Theta} \sum_{i=1}^{n} -\log p(y_i \mid x_i; \theta) - \log p(\theta)$ $\rightarrow$ MLE = MAP iff $p(\theta) \sim U$



\section*{Bayes Optimal Predictors}

Opt dec rule $f^* \text{ knowing } \mathbb{P}(Y \mid X=x)$:
$f^*(x) := \arg \min_{a \in \mathcal{Y}} \mathbb{E} [\ell(a, Y) \mid X = x] = \arg \min_{a \in \mathcal{Y}} \int p(y \mid x) \cdot \ell(a, y) \, dy$

$\rightarrow$ estimate $\hat{p}(y \mid x)$ with $\theta^*_{MLE}$

\subsection*{Bayesian Decision Theory}

Given $p(y \; | \; x)$, a set of actions $A$ and a cost $C: Y \times A \mapsto \R$, pick the action with the maximum expected utility. 

\qquad \qquad $a^* = \text{argmin}_{a \in A} \; \E_y[C(y,a) \; | \; x]$

Can be used for asymetric costs or abstention

\section*{Probabilistic View on Regression}

$y = f(x; \theta^*) + \varepsilon$ 

where $\epsilon \sim \mathcal{N}(0, \sigma^2)$ and $f(x) = w^\top x$:

$ \rightarrow \hat p(y \; | \; x, \theta) = \mathcal{N}(y; w^\top x, \sigma^2)$


\subsection*{MLE \color{black}$\hat p(y \; | \; x, \theta) = \mathcal{N}(y; w^\top x, \sigma^2)$}

$\hat w = \argmax{w} \; p(y |Â x, \theta) =\argmin{w} \sum (y_i - w^\top x_i)^2$

\subsection*{MAP}

Assume prior on $\theta \rightarrow \uparrow \text{bias} \downarrow \text{var}$ 

a) $\theta_i \sim \mathcal{N}(0, \sigma^2_\theta)$ b) $\theta_i \sim \text{Laplace}(0, b)$

Posterior: $p(\theta \; | \; x, y) = \frac{p(\theta) \cdot p( y \; | \; x, \theta)}{p( y \; | \; x)}$

$\hat w = \text{argmax}_w \; p(w \; | \; \bar x, \bar y)$ \\[-8pt]

\quad $= \text{argmin}_w \; \lambda ||w||_{1_b, 2_a}^2 + \sum_{i=1}^n(y_i - w^\top x_i)^2$

a) $\lambda = \frac{\sigma^2}{\sigma^2_\theta}$ b) $\lambda = \frac{\sigma^2}{b^2}$

Priors = regularizers \& likelihoods = losses

% continue here: 
\section*{Probabilistic View on Classification}

$f$ minimizing the population risk: $f^*(x) = \text{argmax}_{\hat y} \; p(\hat y \; | \; x)$

This is called the Bayes' optimal predictor for the 0-1 loss. Assuming iid. Bernoulli noise, the conditional probability is:

\qquad \qquad$p(y \; | \; x,w) \sim \text{Ber}(y; \sigma(w^\top x))$

Where $\sigma(z) = \frac{1}{1 + \exp(-z)}$ is the sigmoid function. Using MLE we get:

\quad \;$\hat w = \argmin{w} \sum_{i = 1}^n \log (1 + \exp(-y_i w^\top x_i))$

Which is the logistic loss. Instead of MLE we can estimate MAP, e.g. with a Gaussian prior:

$\; \;\hat w = \argmin{w} \; \lambda ||w||_2^2 + \sum_{i = 1}^n \log (1 + e^{-y_i w^\top x_i})$






